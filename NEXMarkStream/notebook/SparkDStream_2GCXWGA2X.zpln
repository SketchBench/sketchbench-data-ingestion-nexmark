{
  "paragraphs": [
    {
      "text": "%spark\nsc.getConf",
      "user": "anonymous",
      "dateUpdated": "2021-07-12 23:16:04.069",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkConf\u001b[0m \u003d org.apache.spark.SparkConf@abf7113\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626035270608_1758749163",
      "id": "paragraph_1626035270608_1758749163",
      "dateCreated": "2021-07-11 20:27:50.608",
      "dateStarted": "2021-07-12 23:16:04.087",
      "dateFinished": "2021-07-12 23:16:04.323",
      "status": "FINISHED"
    },
    {
      "text": "%spark\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.SparkContext\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nimport org.apache.spark.util.LongAccumulator\nimport org.apache.spark.streaming._\n\nimport scala.util.parsing.json.JSON\n\nval ssc \u003d new StreamingContext(sc, Seconds(2))\n\nval r \u003d scala.util.Random\nval groupId \u003d s\"stream-checker-v${r.nextInt.toString}\"\n\n    \nval kafkaParams \u003d Map[String, Object](\n  \"bootstrap.servers\" -\u003e \"kafka:9092\",\n  \"key.deserializer\" -\u003e classOf[StringDeserializer],\n  \"value.deserializer\" -\u003e classOf[StringDeserializer],\n  \"group.id\" -\u003e groupId,\n  \"auto.offset.reset\" -\u003e \"latest\",\n  \"enable.auto.commit\" -\u003e (false: java.lang.Boolean)\n)\n\nval topics \u003d Array(\"mytopic\")\nval stream \u003d KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)\n\nval batchInterval \u003d Seconds(5)\n\nval batchesToRun \u003d 10\n\ncase class Message(\n      time: String,\n      auction_id: String,\n      person: String,\n      bid: String\n)\n\nobject FinishedBatchesCounter {\n  @volatile private var instance: LongAccumulator \u003d null\n\n  def getInstance(sc: SparkContext): LongAccumulator \u003d {\n    if (instance \u003d\u003d null) {\n      synchronized {\n        if (instance \u003d\u003d null) {\n      \tinstance \u003d sc.longAccumulator(\"FinishedBatchesCounter\")\n        }\n      }\n    }\n    instance\n  }\n}\n\nval messages \u003d stream\n    .map(record \u003d\u003e record.value)\n    .flatMap(record \u003d\u003e {\n        // // Deserializing JSON using built-in Scala parser and converting it to a Message case class\n        // JSON.parseFull(record).map(rawMap \u003d\u003e {\n        //     val map \u003d rawMap.asInstanceOf[Map[String,String]]\n        //     print(rawMap)\n        //     //Message(map.get(\"time\").get, map.get(\"auction_id\").get, map.get(\"person\").get, map.get(\"bid\").get)\n        // })\n         JSON.parseFull(record).map(rawMap \u003d\u003e{\n            val map \u003d rawMap.asInstanceOf[Map[String,String]]\n            Message(map.get(\"event\").get, map.get(\"event\").get, map.get(\"event\").get, map.get(\"event\").get)\n         } )\n    })\n\n\nmessages.foreachRDD { rdd \u003d\u003e\n\n  val dinishedBatchesCounter \u003d FinishedBatchesCounter.getInstance(sc)\n\n  println(s\"--- Batch ${dinishedBatchesCounter.count + 1} ---\")\n  println(\"Processed messages in this batch: \" + rdd.count())\n\n  if (dinishedBatchesCounter.count \u003e\u003d batchesToRun - 1) {\n    ssc.stop()\n  } else {\n    dinishedBatchesCounter.add(1)\n  }\n}\n\nmessages.print()\nssc.start()  \nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2021-07-13 00:36:27.962",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:76: \u001b[31merror: \u001b[0m\u0027)\u0027 expected but \u0027(\u0027 found.\n                   Message(map.get(\"event\").get)\n                                  ^\n\u003cconsole\u003e:76: \u001b[31merror: \u001b[0m\u0027]\u0027 expected but \u0027.\u0027 found.\n                   Message(map.get(\"event\").get)\n                                           ^\n\u003cconsole\u003e:76: \u001b[31merror: \u001b[0m\u0027;\u0027 expected but \u0027)\u0027 found.\n                   Message(map.get(\"event\").get)\n                                               ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626035545319_1260970793",
      "id": "paragraph_1626035545319_1260970793",
      "dateCreated": "2021-07-11 20:32:25.319",
      "dateStarted": "2021-07-13 00:36:06.110",
      "dateFinished": "2021-07-13 00:36:06.153",
      "status": "ERROR"
    },
    {
      "text": "%spark\nssc.stop()",
      "user": "anonymous",
      "dateUpdated": "2021-07-13 00:25:21.555",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626131775164_684063224",
      "id": "paragraph_1626131775164_684063224",
      "dateCreated": "2021-07-12 23:16:15.168",
      "dateStarted": "2021-07-13 00:25:21.570",
      "dateFinished": "2021-07-13 00:25:45.746",
      "status": "ABORT"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-13 00:25:21.569",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626135921569_832053266",
      "id": "paragraph_1626135921569_832053266",
      "dateCreated": "2021-07-13 00:25:21.569",
      "status": "READY"
    }
  ],
  "name": "SparkDStream",
  "id": "2GCXWGA2X",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": false
  }
}