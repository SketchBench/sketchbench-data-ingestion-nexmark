{
  "paragraphs": [
    {
      "text": "%spark\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.SparkContext\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nimport org.apache.spark.util.LongAccumulator\nimport org.apache.spark.streaming._\n\nimport scala.util.parsing.json.JSON\n\nval ssc \u003d new StreamingContext(sc, Seconds(2))\n\nval r \u003d scala.util.Random\nval groupId \u003d s\"stream-checker-v${r.nextInt.toString}\"\n\nval batchesToRun \u003d 100\ncase class Message(\n      time: String,\n     auction_id: String,\n      person: String,\n      bid: String\n)\n\nobject FinishedBatchesCounter {\n  @volatile private var instance: LongAccumulator \u003d null\n\n  def getInstance(sc: SparkContext): LongAccumulator \u003d {\n    if (instance \u003d\u003d null) {\n      synchronized {\n        if (instance \u003d\u003d null) {\n      \tinstance \u003d sc.longAccumulator(\"FinishedBatchesCounter\")\n        }\n      }\n    }\n    instance\n  }\n}\n    \nval kafkaParams \u003d Map[String, Object](\n  \"bootstrap.servers\" -\u003e \"kafka:9092\",\n  \"key.deserializer\" -\u003e classOf[StringDeserializer],\n  \"value.deserializer\" -\u003e classOf[StringDeserializer],\n  \"group.id\" -\u003e groupId,\n  \"auto.offset.reset\" -\u003e \"latest\",\n  \"enable.auto.commit\" -\u003e (false: java.lang.Boolean)\n)\n\nval topics \u003d Array(\"NEXMarkTest1\")\nval stream \u003d KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)\n\nval messages \u003d stream\n    .map(record \u003d\u003e record.value)\n    .flatMap(record \u003d\u003e {\n        // // Deserializing JSON using built-in Scala parser and converting it to a Message case class\n        // JSON.parseFull(record).map(rawMap \u003d\u003e {\n        //     val map \u003d rawMap.asInstanceOf[Map[String,String]]\n        //     print(rawMap)\n        //     //Message(map.get(\"time\").get, map.get(\"auction_id\").get, map.get(\"person\").get, map.get(\"bid\").get)\n        // })\n         JSON.parseFull(record).map(rawMap \u003d\u003e{\n            val map \u003d rawMap.asInstanceOf[Map[String,Map[String,String]]]\n            val inMap \u003d map.get(\"event\")\n            //Message(inMap.get(\"time\"), inMap.get(\"auction_id\"), inMap.get(\"person\"),inMap.get(\"bid\"))\n            inMap\n         } )\n    })\n\n//messages.cache()\n\nmessages.foreachRDD { rdd \u003d\u003e\n\n  val dinishedBatchesCounter \u003d FinishedBatchesCounter.getInstance(sc)\n  println(s\"--- Batch ${dinishedBatchesCounter.count + 1} ---\")\n  println(\"Processed messages in this batch: \" + rdd.count())\n  \n  if (dinishedBatchesCounter.count \u003e\u003d batchesToRun - 1) {\n    ssc.stop()\n  } else {\n    dinishedBatchesCounter.add(1)\n  }\n}\n\n\n// messages\n//     .map(msg \u003d\u003e (msg.get(\"auction_id\"), 1))\n//     .reduceByKey(_ + _)\n//     .print()\n\n// // Printing messages with \u0027weird\u0027 uids\n// val weirdUidMessages \u003d messages.filter(msg \u003d\u003e msg.get(\"bid\") !\u003d \"100\" )\n// weirdUidMessages.print(20)\n// weirdUidMessages.count().print()\n\n\nssc.start()  \nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2021-07-18 04:21:52.334",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626035545319_1260970793",
      "id": "paragraph_1626035545319_1260970793",
      "dateCreated": "2021-07-11 20:32:25.319",
      "dateStarted": "2021-07-18 04:21:52.388",
      "dateFinished": "2021-07-18 04:21:39.094",
      "status": "ABORT"
    },
    {
      "text": "%spark\nssc.stop()",
      "user": "anonymous",
      "dateUpdated": "2021-07-13 00:25:21.555",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626131775164_684063224",
      "id": "paragraph_1626131775164_684063224",
      "dateCreated": "2021-07-12 23:16:15.168",
      "dateStarted": "2021-07-13 00:25:21.570",
      "dateFinished": "2021-07-13 00:25:45.746",
      "status": "ABORT"
    },
    {
      "text": "%spark\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.SparkContext\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.scheduler._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.util.LongAccumulator\nimport scala.util.parsing.json.JSON\n\nval r \u003d scala.util.Random\n// Generate a new Kafka Consumer group id every run\nval groupId \u003d s\"stream-checker-v${r.nextInt.toString}\"\n\nval kafkaParams \u003d Map[String, Object](\n  \"bootstrap.servers\" -\u003e \"kafka:9092\",\n  \"key.deserializer\" -\u003e classOf[StringDeserializer],\n  \"value.deserializer\" -\u003e classOf[StringDeserializer],\n  \"group.id\" -\u003e groupId,\n  // remove this if your Kafka doesn\u0027t use SSL\n  \"security.protocol\" -\u003e \"SSL\",\n  \"auto.offset.reset\" -\u003e \"latest\",\n  \"enable.auto.commit\" -\u003e (false: java.lang.Boolean)\n)\n\nval topics \u003d Array(\"topic-to-consume\")\n\n// Accumulating results in batches of\nval batchInterval \u003d Seconds(5)\n\n// How many batches to run before terminating\nval batchesToRun \u003d 10\n\ncase class Message(\n      platform: String,\n      uid: String,\n      key: String,\n      value: String\n)\n\n// Counter for the number of batches. The job will stop after it reaches \u0027batchesToRun\u0027 value\n// Looks ugly, but this is what documentation uses as an example ¯\\_(ツ)_/¯\nobject FinishedBatchesCounter {\n  @volatile private var instance: LongAccumulator \u003d null\n\n  def getInstance(sc: SparkContext): LongAccumulator \u003d {\n    if (instance \u003d\u003d null) {\n      synchronized {\n        if (instance \u003d\u003d null) {\n      \tinstance \u003d sc.longAccumulator(\"FinishedBatchesCounter\")\n        }\n      }\n    }\n    instance\n  }\n}\n\n// \u0027sc\u0027 is a SparkContext, here it\u0027s provided by Zeppelin\nval ssc \u003d new StreamingContext(sc, batchInterval)\n\nval stream \u003d KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)\n\nval messages \u003d stream\n    // We get a bunch of metadata from Kafka like partitions, timestamps, etc. Only interested in message payload\n    .map(record \u003d\u003e record.value)\n    // We use flatMap to handle errors\n    // by returning an empty list (None) if we encounter an issue and a\n    // list with one element if everything is ok (Some(_)).\t\n    .flatMap(record \u003d\u003e {\n        // Deserializing JSON using built-in Scala parser and converting it to a Message case class\n        JSON.parseFull(record).map(rawMap \u003d\u003e {\n        \tval map \u003d rawMap.asInstanceOf[Map[String,String]]\n            Message(map.get(\"platform\").get, map.get(\"uid\").get, map.get(\"key\").get, map.get(\"value\").get)\n        })\n    })\n\n// Cache DStream now, it\u0027ll speed up most of the operations below\t\nmessages.cache()\t\n\n// Counting batches and terminating after \u0027batchesToRun\u0027\nmessages.foreachRDD { rdd \u003d\u003e\n\n  val dinishedBatchesCounter \u003d FinishedBatchesCounter.getInstance(sc)\n\n  println(s\"--- Batch ${dinishedBatchesCounter.count + 1} ---\")\n  println(\"Processed messages in this batch: \" + rdd.count())\n\n  if (dinishedBatchesCounter.count \u003e\u003d batchesToRun - 1) {\n    ssc.stop()\n  } else {\n    dinishedBatchesCounter.add(1)\n  }\n}\n\n// Printing aggregation for the platforms:\nmessages\n    .map(msg \u003d\u003e (msg.fp, 1))\n    .reduceByKey(_ + _)\n    .print()\n\n// Printing messages with \u0027weird\u0027 uids\nval weirdUidMessages \u003d messages.filter(msg \u003d\u003e msg.uid \u003d\u003d \"NULL\" || msg.uid \u003d\u003d \"\" || msg.uid \u003d\u003d \" \" || msg.uid.length \u003c 10)\nweirdUidMessages.print(20)\nweirdUidMessages.count().print()\n\n// TODO: catch more violations here using filtering on \u0027messages\u0027\n\nssc.start()\nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2021-07-16 02:07:25.540",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626135921569_832053266",
      "id": "paragraph_1626135921569_832053266",
      "dateCreated": "2021-07-13 00:25:21.569",
      "status": "READY"
    }
  ],
  "name": "SparkDStream",
  "id": "2GCXWGA2X",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}